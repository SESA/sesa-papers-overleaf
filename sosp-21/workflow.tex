%\subsection{Workflow}
\label{sec:workflow}

%https://lucid.app/lucidchart/fc20f7aa-529f-44e7-b9b5-8ed52952f7d5/edit?page=l0EzjAKrzgiu#
\begin{figure}
\centering
\includegraphics[width=1.1\columnwidth]{figures/timeline_chart}
\caption[]{Request timeline}
\label{fig:timeline}
\vspace{-0.25in}
\end{figure}


From an OS perspective we break down network driven processing into stages that allows us to organize and reflect the OS and application interaction with the workload request timeline.  This break down is illustrated in figure~\ref{fig:timeline}.   

\subsection{Quiescent}
Given the packet and transactional nature of network driven services a quiescent period, in which no requests are present at the server, precedes activity on the server. 

\subsubsection{Open Loop:}
In an open loop scenario like a Memcached workload, the external request rate induces an inter-arrival gap that will drive the quiescent period -- longer at lighter loads (lower queries per second (QPS)) and shorter at heavier loads (higher QPS). Largely the arrival rate can be considered independent of the time required to service a request.  System performance is typically evaluated in terms of the 99\% tail latency it achieves.  Moreover, providers often set a Quality of Service (QoS) target, such as  a 99\% tail latency of 500${\mu}$s, categorizing system performance into acceptable and unacceptable.  As Mootaz et al.\cite{mootaz} observed, in latency focused workloads, the "latency-slack" between the mean time to servicing a request and the QoS target creates an opportunity for energy-performance tradeoffs.  In particular, it maybe possible to reduce mean performance by not executing requests as fast as possible and or not processing request immediately upon their arrival to reduce energy consumption while not violating the QoS target.  

\subsubsection{Closed Loop:}
In a closed loop setting, like snapshotting a database to a remote server or video streaming, the arrival rate of the next request, that forms the overall task, depends on how long it takes to service the current request.  However, the quiescent period is bounded by the network round trip time, that depends on  the size of a request and reply packets, and the remote processing time.   System performance is largely evaluated on how fast a server can complete the entire task given a network link speed and a remote system.  Given workload specific characteristics such as the processing required and round-trip costs it is possible, as we will see, to find configurations in which the OS interacts with slowing down to improve both time and energy. We know from previous research in energy proportionality in datacenters, the nature of web-centric applications causes diurnal troughs~\cite{Barroso:2009:DCI:1643608, oldi-study, oldi-pegasus, warehouse-power, energyproportion, WebSearch} and one method with which to increase energy efficiency during these troughs is to maximize the amount of work done. 

Given our goal of studying and explaining the implications of the OS on network driven processing our analysis framework, and evaluation, includes closed loop settings.  In particular, we use a simple ping-pong application to stress OS behavior while varying packet-sizes to reveal how OS path length and path efficiency interacts with slowing down processing, relative to the round trip time observed at the server.  Additionally we use a second closed loop that stresses single core application processing with negligible packet sizes to evaluate if OS structure can impact the application efficiency.  

% Often research tends to categorize close loop settings as either not representative of cloud computing or imply that energy-performance tradeoffs are not interesting due to the hight utilization closed-loop processing can imply.  
\subsection{OS Request Detection}

Fundamental to any operating system is how it detects and schedule processing in response to IO device activity.  At the two extremes are interrupt and poll driven detection.  

\subsubsection{Interrupt driven IO}
Using interrupts has three important implications: 1) Interrupts can be used to transition a processor from a halted state which the OS entered to sleep the processor (at some selected HW sleep state (C-state)) in response to external activity, 2)  Interrupts allow an OS to arbitrate processing across competitive devices and processing in a multi-programmed/multi-device setting, 3) interrupt processing has an inherent performance costs associated with it -- in terms of latency in starting handling of a request, either because of the costs associated with preempting currently scheduled work\cite{intelpaper} or exit penalties associated with the C-state that the processors was halted in\cite{}.  Interrupt processing can also have a negative impact on the instruction efficiency, measured in Instructions Per Cycle (IPC), due to induced micro-architecture hazards such as the inability to prefetch or speculatively execute across an interrupt.

\subsubsection{Poll driven IO}

Most modern high-speed devices expose a memory, if not cache, friendly interface, that permits the processors to read a per-core memory address to determine if the device, such as the Network Interface Card (NIC) has received data that requires processing by the core.  This facility allows software to directly poll the device and initiate software handling without an interrupt.  This approach reduces latency and other performance penalties associated with interrupt driven IO.  However, the period in which the device is checked requires CPU activity and thus limits the ability to halt the cores when there is no work to be done.  In the extreme, a customized OS, supporting a single application can run a poll loop on every core to constantly check for work, conduct the work and then go back to polling for new work and thus never halting the processors due to idleness.   While used in a slightly different context to Kim et al.\cite{hank}, we will refer to this regime as 'no-idle', as it results in a similar energy management strategy albeit in the context of network driven processing.  In general such an aggressive poll approach is assumed to maximize performance by avoiding interrupt overheads and minimizing latency. 

\subsubsection{Hybrid driven IO}
% https://wiki.linuxfoundation.org/networking/napi
A general purpose operating system typically exploits some form of hybrid alternating between using interrupts and polling when servicing high speed NICs. A common strategy is to use interrupts when load is low and switch to polling when load is high and back to interrupts when load reduces.  A general purpose OS, even under sustained high load, bounds the poll phase to avoid the starvation of other devices and computation.  In Linux the framework that implements this hybrid scheme for is called New API (NAPI)\cite{NAPI}.  The first arrival of a packet generates an interrupt which switches the servicing of the device to polling for some budget of packets and time after which the device will no longer be polled and interrupts re-enabled to detect activity on the NIC.  
%The NAPI framework, in addition to NIC processing budgets,  supports prioritization across devices. 
% In Linux the NAPI polls are processed via softirqs.  Checks for and processing of softirqs happens when ever the system returns to userspace or a hardware interrupt exits.

A library OS that is specialized to support the execution of a single application can explore more extreme strategies like the aggressive polling, described above, given that it need not arbitrate the device or cores.

\subsubsection{Interrupt Delaying}
\input{itr_flowchart}
A common feature of modern high speed NICs is the ability to delay the delivery of interrupt when an event such as packet arrival or transmission completion. By manipulating this setting software can limit the minimum time between interrupts or in other words the maximum rate at which the NIC events can interrupt the CPUs. The NIC we use in this study exposes this mechanism via an Interrupt Throttling (ITR) setting.  We flowchart the algorithm from the NIC's datasheet~\cite{82599} in  Figure~\ref{fig:itr_delay_flowchart}. Software uses the ITR register to configure a delay in 2$\mu$s increments.  If the spacing of events, such as packet reception, is less than  $2{\mu}s \times ITR$ the NIC will delay assertion.  If on the other hand events are sufficiently separated an interrupt will be asserted immediately.   

By default the Linux device driver attempts to automatically set ITR to reduce interrupt overheads.  We disable this feature and manually control its value to explore the impact of delaying the detection of NIC events.  Combining interrupt driven IO and delaying interrupt assertion via ITR setting we can explore the impact of delaying packet detection and thus delaying processing of requests.  Depending of the state of the processor the NIC can buffer packets in its own memory or on the main memory of the host\footnote{According to the data sheet if the processor is in sleep state packets can only be buffered on the card's limited memory and once exhausted packets will be dropped}.  Delaying interrupt detection introduces an OS control that can interact with energy and performance.  The prior work of Mootaz et al\cite{mootaz} and the more recently work of Chou et al{chou} suggests that delaying packet processing can interact with DVFS and C-State control in latency sensitive workloads to yield useful energy performance trade-offs. To this end we sweep ITR values for all our workloads OS configurations, and DVFS values.

\subsection{OS Request Processing}

Once the OS detection mechanism identifies that the NIC has data to process several components of OS functionality must be run in accordance with the execution model of the OS.  Specifically device driver code must dequeue layer 2 frames and schedule them for processing by the appropriate network protocol code. This code must ultimately determine the application end point, a port in the case of IP and TCP, and enqueue, the encapsulated data,  for application processing. 

This work, on a general purpose OS, is typically split between two levels of scheduling; 1) interrupt level in which minimal work is done but at the highest critical priority and is run to completion (typically called the top-half processing), 2) the so-called bottom-half uses various kernel facilities to execute both device driver logic and protocol processing in a manner that can be preempted and rate limited.  Regardless all this work is done at the OS privilege domain and ultimately prepares data for the workload specific preemptable    application processing which is independently scheduled at lower privilege and priority. 

Library operating systems that are customized for a specific application and processing model often shed much of the above complexity both shortening the path and eliminating the above privilege scheduling domains\cite{IX,Arrakis,EbbRT}.  Rather they can exploit short-cuts that allow run to completion execution of all the logic, including application, processing in response to detecting device activity.  

\subsubsection{Application Processing}
Once the OS has completed protocol processing it enables application logic to begin.  Depending on the workload this may be very simple as in the case of a workload like memcached or it require significant cpu activity as in the case of  an application server written in managed runtime such as nodejs or one that does non-trivial work to service a request. 

As illustrated, during application processing the OS logic may be interleaved.  This work roughly falls into two categories, synchronous work done in service of this application request (page-faults, system calls, etc) and asynchronous work not having to do with this request (OS background work, processing of other requests or processes).

Library OS's can often avoid interleaving asynchronous work, unrelated to the request handling,  and thus minimize jitter and improve IPC. 

\subsubsection{OS Reply Processing}

As illustrated At some point during application processing a reply is generated and submitted to the OS for transmission.  This can often be handled in an asynchronous fashion depending on the OS semantics.  Where the OS can initiate protocol processing and device transmission in parallel with the remaining application request processing logic (eg. book keeping, cleanup and preparation for the next request).

This potential overlap is an important distinction as it reveals that some applications may have an associated opportunity for a performance/energy optimization.  Specifically, it is possible given a particular arrival rate that there maybe settings that permit the remaining application work to overlap with the time for the next request to arrive in both a closed and open loop settings.  As such it maybe possible that tradeoffs in sleep state latencies, interrupt overheads and polling leads to better performance at lower energy consumption.  

\subsubsection{Idle Policy}

If all processing is complete, no traffic is pending and aggressive polling is not in use the OS can enact a policy that select a hardware sleep state (C-State) to halt the core to.  The sleep states and various policies has been extensively studied\cite{a,b,c,d}.  Each sleep state is has an associated reduction in static power consumption. In the extreme, the deepest sleep states can flush micro-architectural state such as caches and power down these structures. However, each sleep state also imposes a progressively larger wakeup latency and potential impact on execution efficiency given the possible flushing of state. 

If there is space we might want to add the c-state for our processor similar to what was done in Brooks

There is clearly a relationship between the Idle Policy and Request Detection processing.  For a general purpose OS the normative assumption is both are interrupt driven.  Where an inter-dependency between the halt and interrupt mechanisms of the processor is exploited.  

In our study we allow Linux's scheduler and default idle driver to decide if a core should be halted and to what state.  The software exploits various statistics to estimate how long the core is likely to be idle. It takes into account an estimate of when the next interrupt will likely occur from any source.  This is a subtle implementation that interacts across many layers of the OS software including the device drivers.  The idle driver framework also includes code provided by the processor manufacturer to evaluate the latency penalties and suggested minimum residency times core each sleep state.  This allows us to see the impacts of making informed decisions regarding sleep states.  


When we use the Library OS to explore customized behavior we explore two simple configurations.  In the first we when there is no work to process on a core we simply put the processor into the deepest sleep state thus ignoring any tradeoffs in the use of other sleep states.   This allows us to focus on the interaction that slowing down the processor and adjusting the ITR can have with the fixed use of deep sleep.   The second approach we take is the 'no-idle' configuration as discussed above where have no idle policy and simply immediately got back to using poll to implement request detection.

\subsection{DVFS}

As noted the use of Dynamic Voltage Frequency Scaling or a processors allows software to adjust the energy consumption of CMOS based logic while trading off the rate at which instructions complete.  As noted by gernot, brooks, hank and chou static or leakage energy consumption is not particularly affected by DVFS and induces a based cost for keeping a fix core architecture active.\footnote{Opposed to big-little or reconfigurable core architectures.}

Our study focuses on revealing how "slowing down" processing, using DVFS mechanisms, interacts with the processing of network driven software stacks and the resultant energy and performance realized.    From this perspective we will simply view DVFS as a speed control setting that can dilate cpu processing components of the request timeline in exchange for reduction in energy consumption.

From this perspective the three obvious components that can be affected are OS Request, Application and OS Reply processing.  For a given OS their will be a hot-path instruction sequence that will be commonly exercised to process the request packets.  The OS implementation will determine the nature of the instructions that will compose this path for a particular workload.  As such at the fastest DVFS setting their will be a characteristic mean number of cycles that will be required and thus an IPC efficiency realized.  It is important to note that OS IPC does not necessarily imply better or worse performance and energy.   What matters more is the net effect the OS instructions have with respect to application efficiency (eg. request processed per OS cycle). 

As a matter of fact a core finding is that  shorter OS path length with lower IPC can imply better net efficiency.  If the OS can support the application work in shorter number of cycles using less energy it can lead to an improvement in application IPC and energy efficiency.  In an OS dominate workload, a shorter OS path with lower IPC, can allow one to slow down the core and not impact the performance of the OS code as it is not ALU bound. In the case of an application bound workload such an OS path can result in application efficiency due to a reduction in micro architectural hazards and thus magnify the range of power management configurations that can maintain performance.    
 
 
Another less obvious interaction that can occur is between the use of polling versus interrupts and halting.  Given that polling is a CPU operation it can be 

%\subsection{Application Perspective}
%Figure~\ref{fig:timeline} shows the application is waiting to be woken up to process new packets (a). Next, an interrupt (b) is fired and the OS network stack begins processing the received packet (c). The application level work begins, alongside there are also interspersed OS work which may or may not be in direct support of application(d). The tail end of the application work (e) typically entails a response packet being sent, the period of time with which the response packet is physically sent can proceed in parallel with the rest of the application level work. The end of every request handling (f) also revolves a set of OS policies to decide the next state of the software and hardware. Work is time spent executing instructions required to service a request, it is a function of the software, hardware, and workload itself. Whereas, idle time is a function of arrival rate of packets.
%
%\subsection{Hardware Perspective}
%Slowing down of the processor causes an increase in the time spent in portions of application and OS work while reducing energy use. Slowing down of interrupt delays contributes to the increase of time spent in the idle states, the longer a processor idles the more energy it can save.
%
%\subsection{OS Perspective}
%OSes overall behaviour is a function of how it behaves during both the working and idle portions of time, there also exists a clear inter-relationship between the two. 
%
%The amount of energy used during this idle period is dependent upon the OS idling policies; in terms of which level of idle state is selected.
%
%\subsection{Equations}
\input{model}
%\subsection{ITR-Delay algorithm}
