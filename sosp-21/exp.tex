\label{sec:exp}
Figures~\ref{fig:closed_loop_overview}~\ref{fig:mcd_overview},~\ref{fig:mcdsilo_overview} shows an overview of all the experimental datapoints gathered across the different applications and their respective loads as listed in table~\ref{table:wrkcfgs}. For each workload, we break down the differences in slowing down in the respective system types in terms of performance (time for closed-loop and 99\% tail latency for open-loop) and their respective energy use. In order to reason about the trade-offs that occur when slowing down the processor speed and interrupt delay, we use two graphical mechanism to highlight the differences: 
\begin{enumerate}
    \item The \textit{size} of each point is representative of the degree with which the interrupt delay used; the \textit{larger} the size the more interrupt delay is \textit{increased} while the \textit{smaller} the size the more it is \textit{decreased} (faster network interrupts).
    \item The \textit{color gradient} of each point represents the degree of slowing down processor speeds; the \textbf{darker} the color the more the processor has been slowed and vice-versa when the color is \textit{lighter}.
\end{enumerate}

In total, we conducted this study across four system types across the two OSes. For linux and the libOS, the bulk of this work is focused on studying the effects of slowing down processor and interrupts by statically tuning them, and in the figures below, we refer to these datapoints as \textit{LibOS-tuned} and \textit{Linux-tuned}. As a baseline for linux, we tested a configuration where it is not statically tuned (\textit{Linux-default}). For the libOS, we also explored a version of slowing down the processor by replacing network interrupts with a polling loop(\textit{LibOS-poll}). Furthermore, for each of the system type that we've measured, the configuration that yielded the best performance and best (lowest) energy are indicated with a {\larger[4]\textbf{+}} and {\larger[4]\textbf{x}} respectively.

\subsection{LibOS-poll}
The simple run-to-completion, and lightweight event-driven execution model of the libOS allows us to uniquely explore the performance-energy trade-offs of slowing down the processor in the context of a polling loop for packet processing. Intel's 82599 NIC defines two hardware registers that enable automatic clearing and masking of interrupts per receive and transmit queue. To enable polling, we 1) enable auto clearing of network interrupts in order to receive the very first interrupt of every queue, and we 2) disable auto masking to prevent future interrupts on that particular queue. Next, we wrote a custom network interrupt handler for that very first interrupt which effectively calls the packet processing function in a tight loop. To enable interrupts, we enable both interrupt clearing and masking and set the interrupt handler to be the same packet processing function.