\label{sec:intro}
% While many have studied power management mechanisms and control from an architectural or application perspective we approach the subject from and OS research vantage point.  Our goal is not the construction of a particular policy or mechanism rather it is to quantify and explain how and why slowing down processing can be beneficial. Gernot suggests that we must consider entire system. Hank quantifies that one must consider complex tradeoff space in energy management as hardware is nolonger simply race-to-halt friendly -- race-to-halt, pace-to-idle, optimal and no-idle -- due to changes in the dynamic and static components of energy consumption driven by differences in CMOS and SRAM composition and usage in processors. Building on prior work Chou suggests that latency sensitive workloads are different and matter. Brooks suggest that optimal energy management will require workload specific specialization

%Our work extends and builds upon that of Sueur et al. and Mootaz et al.  Sueur et al. noted that to guide an OS designer it is critical to analyses the system as a whole, including the workload, to determine whether using mechanisms such as DVFS [Dynamic Voltage Frequency Scaling to slow processing down] will be effective at reducing energy consumption. While Mootaz et al observed that delaying request processing can result in batching effects that permit power savings while maintaining tail latency requirements for web serving.

%More recently, Chou et al. proposed a dynamic control policy in the context of micro-services that advocates for a mixing both the slowing down of the CPU and delaying processing in a latency sensitive manner. The authors focus is one of policy development with little insight into the impact that the OS's design and implementation has.    

%To our knowledge we are the first to conduct a detailed experimental study that evaluates how the OS structure and choices interact with slowing cores down using hardware power management (DVFS) and delaying interrupt delivery using NIC settings (IDR).  We experimentally gather data for a very large combination of DVFS and IDR setting across four workloads broken down into two categories open loop and closed loop. Each category consists of one workload that is OS centric and the other that is application centric. Further to tease apart the OS impact  in addition to evaluating a general purpose OS (Linux) we use a Library OS, that we can more easily modify to explore a broader range of systemic OS choices.   

%Breaking down request processing into phases we construct a mathematical model that allows us to capture and explain the OS's behavior and its impact on both performance and energy.  The model is both intuitive and consistent with our experimental observations.  

% Building on the above observations we uniquely study performance and energy impact of slowing down both processors and delaying request processing on network driven workloads from an OS perspective. Ultimately with the goal of revealing current behavior and guiding future work in both OS and hardware.

% Using our OS knowledge and prior architectural and application oriented work on power management, we construct a simplified request processing model, that can be expressed mathematically.  We construct the model to reflect what  we believe to be the important interactions that the OS can have on the realized performance and energy when running various types of network oriented workloads.   Specifically,  the model lets us evaluate how slowing down processing and interrupt detection interacts with  variations in the instructions components of OS and Application request processing, the impact of specializing the OS for a single network application, the use of interrupts versus polling and interactions with the use of hardware sleep-states.  Using the model we can predict the general behavior one might observed with respect to both performance and energy.

% We conduct an exhaustive study that sweeps processor slow downs and interrupt delays in the context of four network benchmarks designed to stress different OS and application behaviours.  We evaluate both a general purpose OS and a library OS specialized for to run single network oriented servers.  Doing so lets us vary OS path lengths and efficiencies, differences in interrupt and polling behaviors along with sleep state strategies.
% We then analyze the data to draw validate core observations of how the OS affects the observed combination of performance and energy for the workloads.  We also use the data to evaluate our model.

% We assert that the OS has a bigger role to play in energy management. then perhaps has been realized, by combining its ability to slow down processing, delay interrupts and specialize its paths for network service oriented applications.   Given that it is the OS that controls the core network processing paths, including device management, interrupt versus polling behavior, and halt behavior it is important to understand OS effects in both performance and energy.

% The contributions of this work are:
% \begin{enumerate}
% 	\item one
% 	\item two
% 	\item three
% \end{enumerate}

% Section A, B, C, D.

% First we discuss meaning of timeline
% We discuss how the timeline relates to slowing down w.r.t performance-energy trade-offs
% \subsubsection{Race-to-Halt vs Pace-to-idle vs No-idle}

%Model is a framework for revealing the effects given OS choices in how to use interrupts, halting and polling.  

%The performance and energy consumption of a system are complex emergent properties of a myriad of interactions between application software, operating systems, hardware, and request loads. 
Summary intro: This paper is about how slowing down processor and interrupt delay affect performance and energy of web-centric applications. Also, how do different OS structures affect this as well.

Our work seeks to broaden the understanding of web-centric applications running under different OS structures and their performance-energy trade-offs in this space; furthermore, we are interested in how the mechanisms of slowing down both processor and network request processing, by interrupt delay, affect performance and energy. We did a exhaustive search through this in both Linux and a library OS on different applications with different request loads.

From our experimental analysis, we summarize the following observations and contributions in our work:
\begin{enumerate}
    \item \textbf{No single strategy for OS to optimize energy and performance:} in both open and closed loop workloads, we find at lightweight request loads, it is actually better to use a OS poll than interrupts; we find that an OS poll improves performance and save energy; these improvements can be as much as 3X. (\cref{sec:closed_loop:poll} \cref{sec:mcd:poll}) 
    \item \textbf{Slowing down processing and interrupt delays can reduce energy use in a workload dependent fashion:} we find this is applicable across both OS structures and careful manual control of both delay mechanisms can result in energy savings up to 2X across the workloads and OSes. (\cref{sec:closed_loop:speedup}, \cref{sec:mcd:slowprocvssleep}, \cref{sec:mcdsilo:dvfstradeoff})
    \item \textbf{Specializing the software magnifies the energy benefits of slowing down:} we find a specialized OS stack can more aggressively slow down processing to save energy by up to 76\%, with no noticeable impact on performance, we attribute this to: a) shortened OS path length can get same application work done using fewer instructions, and ii) given that resultant instruction mix are not ALU bound, therefore causes minimal time penalties with a slowed processor. (~\cref{sec:mcd:slowinos})
    \item \textbf{Specializing the software creates headroom for trade-offs, even in application heavy workloads:} we find a specialized OS can reduce time spent in the OS processing by improving application code IPC; this creates additional headroom to use both delay mechanisms to further reduce energy use by 6\%-43\%. (\cref{sec:mcdsilo:ipc}) 
    \item \textbf{A mathematical framework that can be used to explain and explore software and hardware effects of slowing down:} we cross-validated our framework with experimental results and discuss potential implications towards better policy designs. (\cref{sec:model})
\end{enumerate}

We begin by breaking down a generic timeline of how packets are processed in a typical system in \cref{sec:workflow}, this breakdown will help seed the discussion of how different system components are affected by slowing down mechanism in \cref{sec:slowdown}. Next, we discuss our experimental and software setup in \cref{sec:exp_setup} and then go into a deeper analysis of our experimental data in \cref{sec:exp}. We discuss where our work sit in literature in \cref{sec:related} and end with a discussion of implications and future work in \cref{sec:dis} and finally conclude in \cref{sec:conc}. 

% %% TODO: probably move to intro
% \begin{em}
% Core Finding: 
% \begin{enumerate}
% 	\item There is no single strategy for OS control to optimize energy and performance.
% 	%% polling is better in some instances 
	
% 	\item And yet the OS has sufficient control through slowing down processing and delaying interrupts to achieve dramatic wins in a workload dependent fashion. (give a couple of examples)
% 	%% applicable to both general purpose and specialized OS
	
% 	\item Further specializing the OS for running a single network driven service on a node magnifies the benefits of OS control to optimize energy consumption. 
% 	%% i.e. memcached -> libOS trade-offs different than linux, even as QPS gets larger
% 	%% as os designers, specializing is worth it for the
% \end{enumerate} 
% \end{em}

% \begin{em} 
% A core finding is that indeed, in the case of open loop benchmarks OS control of the NIC interrupt delay can be used to slow down request processing and allow one to exploit the latency slack to achieve lower power consumption while maintaining a QoS target.  This control is effective enough that it can server as knob by which the 99\% tail latency target can be selected and energy can be minimize to that target.  In the case of closed loop workloads it can act as a control that allows one to optimize protocol processing to minimize energy while maximizing performance.
% %% to what extend will it impact OS
% \end{em}

% \subsection{Interaction with Specializing OS Paths} 
% \begin{em}
% Critically we find that optimizing the hot-path of network processing in an OS can lead to valuable energy savings.  If the OS can support the application work in shorter number of cycles using less energy it can lead to an improvement in application IPC and energy efficiency. 

% In an OS dominate workload, a shorter OS path (even if it means lower IPC), can allow one to slow down the core and not impact the performance.  We find that specialized OS networking paths can have lower IPC, due to the nature of the instruction mix, and thus have performance that is less affected by lower DVFS setting and yet still have the benefit of lower power consumption.  In some sense this is the counter point to the extreme IPC efficiency that can be had by running loop of NOPs.    The core point is despite having lower IPC more application work, in this case network processing, is being completed in the same amount of time but consuming less energy.
% %% by optimizing OS path, created a path with 2 benefits: 1) get more work done, 2) instructions less sensitive than DVFS, not ALU bound, i.e. memory-bound instructions likely

% In the case of a service oriented workload that has significant application computation, such that then fraction of the instructions composed by the OS network processing is small, there is still a potential for improved performance and energy tradeoff.  Customized OS paths can both reduce the time spent in the OS processing and improve the application code IPC due to a reduction in architectural hazards associated with interrupts, protection domain crossing, etc.  This time reduction, effectively increases the headroom in which the benefits of either slowing down processing and or delaying interrupts can be used to find an optimal setting for the workload. 
% % mcdsilo
% \end{em}   
 
% \subsubsection{Polling can be energy efficient}
% Another less obvious interaction that can occur is between the use of polling versus interrupts and halting.  Given that polling is a CPU operation it can interact with DVFS settings.  In particular it can be possible that at slower DVFS settings polling can be more efficient and effective with respect to the net performance realized.  

% \begin{em}
% A core finding is that aggressive polling can be combined with slowing down processing to find a setting that yields best case performance at a lower energy consumption for some workloads.   An example is when the workload is closed loop and transmission times are small. 
% \end{em}


%Gernot et al.  

%The hardware nodes of a cloud service provider are often each dedicated to running a single cloud service component~\cite{FB}.
%As latency-critical tasks become ubiquitous across data centers,
%deploying them on dedicated nodes is becoming a well studied and favored decision~\cite{ixcp, heracles, PerAppPower, twine}.
%Typically, this dedication prevents latency violations that might be triggered
%by the co-location of best-effort batch tasks.
%One hopes that this dedicated nature (i.e. fixed role in the service using a fixed software stack) can be exploited to obtain the required performance (e.g. 99\% tail latency) while minimizing the energy used, a key concern given the increasingly constrained energy budgets in data centers~\cite{ixcp, SmoothOperator, Dynamo, oldi-study, oldi-pegasus, NLP-energy}.

%General purpose OSes such as Linux have been designed to support a range of user software as well as the concurrent execution of competing applications, thus, they have evolved to include support for dynamically adjusting various hardware settings on modern CPUs and network interface cards (NICs). Past researchers have demonstrated that dedicating a node to a single application can attain dramatic performance gains~\cite{ix,arrakis, exokernel,ebbrt,rumpkernel, unikernels, aliraza}, these results also suggest that one may be able to cater a node's hardware parameters to obtain higher efficiency than allowing the OS to dynamically adjust them.

%While researchers have proposed application specific OSes in the past, their optimizations have mainly focused on OS level changes targeting performance.

%However, dedicating a node to a single application suggests that one may be able to manually tune the node's hardware settings to magnifiy the impact of such tuning. While researchers have proposed such systems in the past, referred to as library OSes~\cite{ix,arrakis, exokernel,ebbrt} or unikernels~\cite{rumpkernel, unikernels, aliraza}, this 
%Dedicating a node to a single application suggests that one may be able to manually tune the node's hardware parameters to fixed values and obtain higher efficiency than allowing the OS to dynamically adjust them.  Furthermore, it is possible that the impact of hardware tuning can be magnified if one starts with an application specific OS that has been designed and implemented for running a single dedicated application. 

%However, the performance and energy consumption of a system are complex emergent properties of the myriad of interactions between application software, operating systems, hardware, and offered loads. Interestingly, when we take a careful look at overall system execution under different energy profiles, we see that the consequences of tuning energy and performance reach far beyond directly observable quantities and impact the interactions between the aforementioned system components in subtle ways. There is a dizzying array of operating system mechanisms and policies one must consider that could both individually and collaboratively impact the performance and energy use of the system. Rather than focusing on the design and implementation of these mechanisms and policies, we find that we need a greater understanding of their underlying impacts and dynamics on energy and performance.

%The trade-offs that exist in application performance and energy is a well studied field: the typical method is by slowing down processor frequencies such that application performance suffers, but with the added benefit of lower energy use~\cite{7349225,rapl2015, rapl2018,hotpower2008, PerAppPower}. These studies typically study Dynamic Voltage and Frequency Scaling (DVFS) and Running Average Power Limit (RAPL) knobs that are provided by modern processors. Furthermore, they also mainly focus on processor and/or memory-centric applications where a slowing processor frequency has a direct impact on application performance.

%In contrast, datacenter applications tend to have more headroom in this trade-off space as they are typically governed by Service-Level Agreements (SLAs) that provide a ceiling of performance that majority of requests must meet. There have also been a wealth of research in using these SLA headrooms to lower datacenter energy use mainly by decreasing processor frequencies~\cite{Dynamo, SmoothOperator, oldi-pegasus, adrenaline, heracles, energyproportion, warehouse-power}.

%In this paper, we study an additional parameter, along with processor frequencies, to further slow down network driven workloads - which is the time delay in interrupt notification of the device driver to start processing packets (ITR-Delay). Figure~\ref{fig:itr_delay_flowchart} is a transcription of the actual algorithm in the Intel 82599 NIC datasheet~\cite{82599} used in this study. 

%\textbf{Basically, all this can be cut if we replace with summary of findings.}
%While the algorithm itself is simplistic, it has interesting implications on application performance and energy, such as packet coalescing and sleep state usages - \textbf{elaborate.} Furthermore, it brings in new sets of questions of effectiveness when used together with lowered processor frequencies - \textbf{elaborate.}

%Moreover, this paper broadens the study of performance-energy trade-offs of slowing down network applications from previous research by conducting a study in both a baremetal Linux and a library OS (LibOS). The system structure of the LibOS' run-to-completion model enables us to explore both a interrupt-driven and poll-based method for running network applications. The poll-based LibOS allows us to explore slowing down of processor frequency as well. There are three main motivations for this: 1) Understand effects of slowing down given different OS system structures, 2) how do OS policies affect this trade-off, and 3) what insights can be gleaned to make better policy designs and do they differ dependent on the OS.


%\input{itr_flowchart}

% There are benefits to slowing down network driven workloads.

% Two types of slowing down:
% \begin{itemize}
%     \item Slow down when to process packets from NIC
%     \item Slow down processor clock speed
% \end{itemize}

% How do application behave under two types of network driven workloads:
% \begin{itemize}
%     \item Close-loop: Increase utilization of machines during diurnal troughs, the system controls the amount of admitted work. Faster time to completion == lower energy
%     \item Open-loop: Focused on tail latency combined with SLA objectives
% \end{itemize}

% What are the types of benefits:
% \begin{itemize}
%     \item A combination of lower time + lower energy to complete some fixed work + (trade-offs that exist within this combination)
%     \item A combination of tail latency + lower energy while meeting SLA objectives + (trade-offs that exist within this combination)
% \end{itemize}

% How is slowing down beneficial dependent on OS structure (os path length):
% \begin{itemize}
%     \item Linux - monolithic kernel, interrupt driven
%     \item LibOS - unikernel, run-to-completion, interrupt driven
%     \item LibOS - unikernel, run-to-completion, poll-based
% \end{itemize}

% Further questions and discussions:
% \begin{itemize}
%     \item Race-to-halt vs slowing down. Right now it is heuristics/black box, can this paper help to say when you should do this and why?
%     \item Why is this benefit useful for the system policy designer/os designer? You should do this or that
%     \item If we care about performance, why not just poll? Why is polling not enough?
%     \item Having different ways to trade off performance+energy is a useful tool to have
% \end{itemize}

% \begin{itemize}
%     \item Slowing down processor frequency to save energy is a well studied field.
%     \item The combination of adding in slowing down of when to begin process network interrupts along with processor frequency is less known.
%     \item What are the implications on benefits of idling in this case? 
%     \item The main goal of the data collection and analysis in this work is to gain insights into the energy and performance interactions of slowing down both in terms of processor frequency and network interrupt delays and how those insights can guide better policy designs in optimizing performance and energy
% \end{itemize}
