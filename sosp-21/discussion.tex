\label{sec:dis}
This study involved conducting tens of thousands of experiment runs resulting in approximately four terabytes of data.  Summaries of the data along with all our software is available on github.  Our data includes fine-grain interrupt energy timelines not covered in this paper.  We are in the process of releasing a Python Dash board that allows any researcher to access and navigate our entire data set.  We believe that this paper only scratches the surface of what can be distilled from the data.

% cut if needed
Our study has taught us not only the value of considering energy in OS design and implementation but also demonstrated the ease and value of always gathering energy consumption data.  We encourage all OS research to explore framing all performance results in context of energy as advocated for by mudge~\cite{917539}.

% cut if needed
Our data and it exhaustive sweep based approach reveals several opportunities to construct alternative OS energy management strategies.  For example we now see how valuable it might be to identify when too switch from aggressive polling back to interrupts.  Or how we might switch from using low DVFS settings while on OS paths and then elevate to higher DVFS settings for application code for the application centric workloads.  Additionally it is clear that the phenomena, arising in real settings, of instruction mixes having differing DVFS sensitivities deserves further investigation. 

% cut if needed 
We have presented our mathematical framework in hopes to encourage other researches to continue to build upon it so that both hardware and software researches can identify how changes in behavior can enable better performance while reducing energy.  In appendix~\ref{sec:appendix} we discuss some of th next steps to be take in the development of the framework and usage. 

% cut if needed
Our work also suggests several immediate next steps to enhance our data and mathematical model.  For example it is worth using the library OS to explore a configuration between aggressive polling and interrupts.  In particular using the ability to have the NIC directly awaken a core by writing to a cache-line rather than using an interrupt.   Similarly, the use of different fixed and dynamic sleep states with the library OS.   And finally adding data that documents the behavior of changing DVFS settings at the boundary of OS and application code.  


%\begin{itemize}
%    \item energy data should be elevated and always shown next to performance data, advocated by mudge~\cite{917539}
%    \item all data is on github, we built a dynamic Python Dash web app to look at data
%\end{itemize}
%
%future work:
%\begin{itemize}
%    \item cacheline-based halting
%    \item complex dvfs use when in os and application
%\end{itemize}

% \begin{itemize}
%     \item ITR-delay can be extended up to 1 ms, effect of this on SLAs with 1 ms budget instead?
%     \item As many have observed there is a tension between using controls to throttle processing and thus energy consumption versus the saving that can be had by finishing work quick and halting into lower energy consuming processor sleep states between the requests for work.  The later strategy is often referred to as "race-to-halt" (r2h) while we will refer to the former as "slow-to-stay-busy" (s2sb).  We found that these behaviours, and their effectiveness are largely emergent due to interactions between various interrupt and polling mechanisms and policies.  Additionally, we find that it is possible for a system to effectively modulate between both and that this ability allows the OS to again exploit a wider range of hardware settings over which behaviour can be optimized.
%     \item While it can be very hard to predict how the various hardware setting and software policy modules will interact, there can exist virtuous relationships that can be exploited. A system has many different settings and behaviours that directly on indirectly affect the energy-performance profile.  It is natural in a complex OS for a set of mechanisms and policies to be constructed for a particular category of hardware settings.  Case-in-point being the sleep state that the processor will enter into on execution of the halt instruction.  Our processor defines several such states, each offering a different tradeoff in power consumption and penalties for entry and exit.  Not surprisingly, several mechanism in Linux inter-operate to estimate when to halt and to what sleep state -- including work on the interrupt path used to create an estimate of interrupt arrival and the load induced.  We found, however, that if your OS paths are simple then a fixed halt to deepest sleep state and poll strategy can be sufficient.  The penalty of using the deepest sleep state can be mitigated by modulating the number of halts required by using the processor's throttling controls.   This in turn means that you need not have the complexity added complexity of sleep state management making your system simpler and reducing the number of control points.  But of course to do this you must have the headroom in processing done on every interrupt.   We believe other such opportunities exist ...
% \end{itemize}

% We are the first to quantify the impact of using library OS software on energy.  We find that as expected the library OS uses fewer instruction to complete the work and that this has impact on the energy consumed.  This largely translates into getting more application level work done in fewer instruction and this matters despite the heavy IO nature of the workloads.  This is in part due to the nature of high speed networking.  High speed data center networks put OS device and protocol processing code on the hot path -- in some sense making the workload actually more compute bound that one might expect.  Using fewer instructions (and attendant drop in busy cycles) to complete the application work leads to two energy consumption benefits; 1) it reduces the base energy cost to complete the work, and 2) it creates greater opportunities to halt the processor between network transactions.  The later benefit makes it possible to achieve a simple "race to halt" behaviour across epochs of network activity.  
% Simplistically, by rapidly and efficiently finishing the work required for a network request and sending the reply, one has the opportunity to halt the processor and enter a software specified hardware sleep state\footnote{Processors typically support a range of sleep states where deeper sleep states result in less energy consumption but have higher latency and potential performance penalties when waking up.} till the next interrupt.  The actually, period that one can sleep for depends, as expected, on the workload and the optimization criteria being imposed (eg. latency versus throughput).   

% We also find that across all our workloads, the simplified and naive policies of the library OS, afforded by not needing to run or arbitrate multiple applications, leads to exploiting deeper sleep states while also achieving higher performance.  In EbbRT, when an interrupt occurs, like other library OS it implements a default run to completion model, by disabling interrupts.  When there are no work,  pending interrupts or packets dequeued from prior interrupts to process, EbbRT simply halts the processor to the deepest available sleep state. In contrast, even though we disable Linux's mechanisms for setting the parameters there are other complex algorithms and policies that affect its behaviour.  For example Linux application processing runs with interrupts enabled, it implements a complex algorithm for enacting a hybrid polling versus interrupt processing across network devices and has a subtle infrastructure for deciding what sleep state should be used for halting the processor based on estimation of when the next interrupt will occur from any source.  This all contributes to a complex dynamic behavior with respect to when the system should halt and if it does halt what sleep state will be requested.  We observer that even after we use fixed values for the three  hardware setting considered the other complex behaviours of Linux limits the ability to tune its combined energy and performance compared to EbbRT.


% We identify three hardware settings 

% The components of cloud services such as cache servers, javascript webservers, and in-memory data bases, are  by their nature are network driven.  induce vary degrees of cpu work.  Their operation and thus their efficiency is a complex mixture of the behaviour of the system software and the application software.  While Network Interface Cards (NICs) have many adjustable parameters, Interrupt Delay, the amount of time the NIC will wait before signaling the OS device driver is an value that can easily be changed without any modification to the OS including the device driver.   As expected for a fixed workload this parameter can critically can influence the behaviour of the software for a given workload.  Delaying the packets, depending on the workload the behaviour of the software, one can vary the degree of batching of packets at the expense of increasing the latency of beginning processing a packet. Intuitively, it is possible for a particular workload and software stack their maybe a fixed value that not only optimizes performance but affects the energy consumed as the tradeoff between the frequency of waking up and the amount of work to process on every wakeup can directly influence the efficiency of providing service.  

%  While library Operating systems are one way of catering to a single dedicated application it maybe possible to integrate such support into a general purpose os by adding a new "single app" kernel configuration that disables or sheds run-time complexity and dynamic behaviors to obtain the same benefits we observe.   