%Our analysis also provides us with a number of surprising Findings:
%\begin{description}
%\item[F1: OS Path Length Impacts] The number of instructions spent on OS functionality matters not just for performance but has subtle and significant interactions when it comes to energy. 
%In Netpipe and MCD where the majority of processing is OS functionality we see that the number of instructions required on the libOS is between X and Y less Linux.  More importantly though shortened OS paths lengths allow a greater opportunities for exposing and exploiting gaps in processing in which the processor can be halted or slowed down.  This leads to enabling a greater range of ways in which the hardware settings can be used to optimize the emergent energy-performance profile. 
%\item[F2: OS impact on Application CPI] The OS behaviour in managing and scheduling work in response to request arrival can significantly improve application CPI.  We see that for NodeJS and and MCD-Silo, workloads where the application processing out weighs the OS, the execution model of the library OS results in a X-Y percent improvement in CPI compared to the best Linux results of the workloads.  This improvement in CPI like F1 improves the fidelity how the hardware settings can be used to achieve improved behaviour.
%\item[F3: Energy adaptation] As many have observed there is a tension between using controls to throttle processing and thus energy consumption versus the saving that can be had by finishing work quick and halting into lower energy consuming processor sleep states between the requests for work.  The later strategy is often referred to as "race-to-halt" (r2h) while we will refer to the former as "slow-to-stay-busy" (s2sb).  We found that these behaviours, and their effectiveness are largely emergent due to interactions between various interrupt and polling mechanisms and policies.  Additionally, we find that it is possible for a system to effectively modulate between both and that this ability allows the OS to again exploit a wider range of hardware settings over which behaviour can be optimized.  
%\item[F4: Virtuous Interactions] While it can be very hard to predict how the various hardware setting and software policy modules will interact, there can exist virtuous relationships that can be exploited.    A system has many different settings and behaviours that directly on indirectly affect the energy-performance profile.  It is natural in a complex OS for a set of mechanisms and policies to be constructed for a particular category of hardware settings.  Case-in-point being the sleep state that the processor will enter into on execution of the halt instruction.  Our processor defines several such states, each offering a different tradeoff in power consumption and penalties for entry and exit.  Not surprisingly, several mechanism in Linux interoperate to estimate when to halt and to what sleep state -- including work on the interrupt path used to create an estimate of interrupt arrival and the load induced.  We found, however, that if your OS paths are simple then a fixed halt to deepest sleep state and poll strategy can be sufficient.  The penalty of using the deepest sleep state can be mitigated by modulating the number of halts required by using the processor's throttling controls.   This in turn means that you need not have the complexity added complexity of sleep state management making your system simpler and reducing the number of control points.  But of course to do this you must have the headroom in processing done on every interrupt.   We believe other such opportunities exist ...

  
%\end{description}
%1)  for compute sensitive workloads, the libOS results in surprising changes to the application CPI,
%2) under different loads or workloads, dramatically different system behaviors can be optimal; where racing to halt may minimize power use in some scenarios, while in others...
%3) the short paths, direct dispatching from interrupt, and non-preemtability of the libOS, enables efficient behaviors for some workloads that don't appear to occur with Linux.  in particular... slow to... 

%To study and effectively identify the role the OS plays, in affecting the global energy-performance behaviour, we found it critical to use an interrupt-centric methodology.  This is due to three main factors; 1) The OS's interrupt and polling behaviour critically  partitions time into busy and idle 2) the OS directly controls the energy consumption behaviour when idle and 3) it's design and implementation can have a significant impact on the instructions and their efficiency that compose the busy time.  As such we have found that the local behaviour, demarked between interrupts, of the OS combines to significantly influence the global energy-performance behaviour.  Typical approaches such as periodic PC sampling would overlook or washout these critical OS episodes. 

%Our use of fine-grain data logging, on each interrupt has allowed us to break-down and analyze the OS behaviours in the library OS that  causally contribute to the realized energy-performance profile.
%As a matter of fact,  Using our interrupt based logs we have found that:
% \begin{description}
% 	\item[streamlined OS interrupt paths]
% 	\item[single protection domain]
%    \item[run to completion] strict use of event-driven non-preemptive processing 
%    \item[aggressive halt and simple poll] a simple halt to deepest sleep state on idle policy along with a fixed poll count directly implemented within a device driver
%\end{description}

%While we don't have direct evidence that confirms the benefit we believe that compiling the OS functionality and the app code together to produce a single optimized binary can  better integrate and optimize application processing with the interrupt logic (our prior studies have indicated a benefit of both compile time and link time optimization  on library OS function dispatching).  

%
%\include{osdi}

%
%%Key insights from this data include:
%\begin{itemize}
%\item The hardware setting we originally selected for performance (ITR), also has a dramatic impact on energy usage; in many cases greatly dwarfing the importance of the power (DVFS \& RAPL) settings.
%\item For the right workload, using DVFS \& RAPL to slow down the processor can resulting in improving performance; for example, by   
%\item It is critical to manage these hardware settings in a coherent fashion; 
%\item A libOS model for running a single application, not only dramatically improved performance (up to 3x for x)  but can achieve this with much lower energy use.  
%\item Statically configuring linux does better in both performance and energy when running a 
%\item For compute sensitive workloads, the libOS results in surprising changes to the application CPI.
%\item Under different situations, very different behaviors can be optimal; where in some cases racing to halt minimizes power use, while in other situations ...
%\item The short paths, direct dispatching from interrupt, and non-preemtability of the libOS, enables efficient behaviors for some workloads that don't appear to occur with Linux.  in particular... slow to... 
%\end{itemize}
%
%These insights have given us a number of insights on our system, and suggest optimization opportunities that we would have never hypothesized without this analysis.
%Additionally, we believe that they naturally raise a number of fundamental questions about how our operating systems should be designed and controlled given the ever increasing number of settings, controls and policies.
%impact on opportunities for designing our system.  
%indirectly, they raise fundamental quesitons about how OS should be structured. 
%
%in the next section we describe the experiment. three...





% JA: I like a lot of this lets think about the right place for it to go. 
%% We summarize the main findings/contributions of this study below:

%% \begin{itemize}
%% \item \textbf{The OS matters and its effects on tuning headroom.} Not only is there a win when optimizing the settings for a particular workload given an OS software stack the libOS' tuning behaviour is almost always better (see Figures~\ref{fig:netpipe8Kov}-~\ref{fig:mcdsiloov}).  While there is overlap in the energy and performance achieved, tuning the two OS stacks leads to distinct behaviour with the libOS realizing better results both with respect to time and energy. Partly this is also due to the libOS being always able to set DVFS and RAPL lower than Linux.

%% % YA TODO - citation for this
%% \item \textbf{slow-to-stay-busy (stsb)} Past researchers have observed there is a tension between using controls like DVFS and RAPL to throttle energy consumption at some budget versus the savings that can be gained by finishing work quick and halting into idle states between the requests for work~\cite{} - this latter strategy is often referred to as "race-to-halt" (r2h). In this work, we present a new effect of using processor frequency tuning as a way to "slow-to-stay-busy" (\texttt{s2sb}), we found \texttt{s2sb} is only uniquely applicable in the run-to-completion model in the libOS. This was discovered by noticing dramatic decreases in number of interrupts (up to 100X) in NodeJS as DVFS was set to a very low processor frequency. Moreover, we found the bytes transmitted and received did not differ as dramatically from Linux. Upon closer examination of the libOS' receive path, we found the effect of a low DVFS setting is such that on the firing of a receive interrupt, the payload goes through the network stack, application and eventually transmits a reply back. It then has to rewind all the back to the original receive handling code, the lowered processor frequency causes this entire process to be slowed down. Moreover, the libOS' receive code is written such that it will poll the NIC for additional packets up to a certain limit, this limit is based upon values used in~\cite{arrakis,debianixgbe}. Therefore, once the code has slowly returned to the receive interrupt handler, it is possible that new packets have already arrived from the client-side and the libOS begins the process anew. 
  
%% \item \textbf{Packet processing path length and instruction efficiency effects.} A core proposed value for application software stacks is a reduction in the path lengths required to do the requisite processing. For the low CPU requirement workloads, we found CPI to be a mixbag for tuned Linux and the libOS typically had higher CPI than Linux. We found in the high CPU requirement workloads, tuning Linux and the libOS is able to achieve lower CPI than Linux default by up to 20\% for the former and 31\% for the latter. For tuned Linux, this is partly due to the effect that ITR delay has on when network interrupts are fired, and it subsequently impacting the idling manager in Linux (see Section~\ref{}). For the libOS, it is both an effect of ITR delay and interaction DVFS/RAPL on the \texttt{s2sb} effect described above, which implies a reduction in costs to run the interrupt handler code. These surprising result suggest that in contrary to conventional understanding, OS behavior can have substantial impact on the efficiency of even the time spent in application compute. That is, removing boundary crossing, running application work to completion, and dispatching application code directly from an interrupt, may have a secondary effect on the efficiency of that applciation code that was not previously observed. 
  
%% \item \textbf{Energy and Performance Product (EPP) as an analysis indicator.} To measure and understand relative energy and performance savings, the closed loop experiments measure total time to accomplish a fixed amount of work and the open loop workloads use the measured per-request 99\% tail latency while under a specific load generated by a set of client nodes. We take the product of these time indicators with the measured joule usage to derive the EPP value ($Joules \times Performance$) . The relative value of one point versus another on the energy-performance curve depends on what value/utility one places on the importance/cost of time versus energy (see Figures~\ref{fig:netpipe8Kov}-~\ref{fig:mcdsiloov}). Although EPP is one simple way of comparing individual configurations, it is also important to consider the full shape of the energy-performance curve that a system can achieve. A system with only one point with at a lower EPP may not be as useful as a system that has many distinct points at the same EPP - having many points with equal EPP allows tuning for one's utility and cost or changes in utility and costs.

%% \item \textbf{NIC interrupt delay value as tool for reducing time and energy.} We introduce tuning the NIC's interrupt firing delay value as a new knob to tune network driven workloads. We find its benefits at the closed loop experiments to be: for 1) NetPIPE the ITR delay value that gives the minimum EPP is reflection of the resonance between the time to transmit the payload over the wire and a processing time on the client-side (Section~\ref{sec:netpipe}, for 2) NodeJS, a low ITR delay induces a form of polling in both Linux and libOS by preventing the core from going into \texttt{halt} state, which in turn maximizes instruction efficiency to process each request (Section~\ref{sec:nodejs}). In the open loop experiments: 3) we found Memcached can either set its ITR delay exceedingly high (close to SLA) and minimum joule usage, as result producing a high 99\% tail latencyor use a combination of ITR delay with DVFS/RAPL to trade off joules and tail latency more evenly (Figure~\ref{fig:mcdov}) , and for 4) Memcached-Silo, we found ITR delay had a lesser effect as the bulk of work was in database processing where DVFS/RAPL played a bigger role (Section~\ref{sec:mcdsilo}).

%% \item \textbf{Transmit complete interrupts.} While we did not quantify its exact benefits, its existence should be some additional food for thought. Linux's network device interrupt handling code is written in a event agnostic manner, as such it is bound to both new receive packet and transmit completion events. The interrupt handler code will 1) try to clean up any transmit resources (i.e. free memory/descriptors) and then 2) proceeds to process additional received packets sitting in the device. The implication of this effect is that certain ITR delay values could be set such that upon the completion of a transmit the device already has new packets to be processed and can therefore speed up response time without waiting for the next receive interrupt.
  
%% \item \textbf{Exposing software/hardware interactions with a log-based analysis tool.} While using the energy-performance landscapes can uncover general hardware and system trends, they do not expose details of the hardware settings nor other metrics associated with a particular datapoint or what is changing from point to point. It is also less straight forward to expose the underlying interactions between OS structures and hardware tuning. We find that interactive exploration, plotting landscapes of other values and and use of detailed time series plots of the raw logs is necessary to fully understand these system-level interactions. To aid in this, we have built a custom web exploration platform using Python Dash, interested readers can use the following link to explore the datasets we've collected \footnote{To be available soon}.

%% \end{itemize}





