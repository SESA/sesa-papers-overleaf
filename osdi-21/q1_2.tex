
%\subsubsection{Does length matter?}

\paragraph{Length matters(Q1):} Perhaps unsurprisingly, the use of a library OS, designed and implemented to run a single dedicated application, results a dramatic reduction (50-75\%) in the number of instructions required in the two OS dominate workloads, Netpipe and Memcached across all packet sizes, when configured for the best EPP (minimum time and energy).   
To better understand this effect we focus on the Memcached results and utilize the detailed data from one run of Linux and the libOS when then achieve their best EPP (subplots 3b, 4b, 5b and 6b).   

Figure 3b, a plot of the energy time line of a best EPP run for both Linux tuned and the libOS and a Linux default, clearly shows that here is a clear reduction in both tail latency (56\% of Linux Tuned) and the energy consumed (40\% of Linux Tuned).  This translates to a 74\% EPP reduction. The plot also shows that the rate of energy consumption is lower in the library OS. 

%todo: this paragraph can go
Examining the aggregate metrics plot 4a corroborates that indeed there is a dramatic (65\% reduction) difference in the number of instructions used to complete the work. 
Unsurprisingly when statically setting Linux to the values we found to produce the optimal behavior there is not substantial  difference in the number of instructions that Linux requires to do the work.

\vspace{-.2in}
\paragraph{OS Adaptation (Q3):} 
What is surprising is that while the Library OS has a reduced instruction length its instruction efficiency (Cycles Per Instruction, CPI) is actually worse at its best EPP. 
So, while it saved 65\% in instructions, it actually only saves 24\% of the time executing instructions.  
This indicates that there is another effect that is resulting the the large EPP savings we see. 
%If this was the only savings then we would not expect the as large a saving in EPP we see.  

%The impact of the reduced path length induces a fascinating effect.
When we examine  the time between interrupts, we see that the mean non-idle time is approximately 24\% lower than that of Linux tuned.  
However, while the systems handle approximately the same number of interrupts, the number of times the library OS halts is dramatically lower than that of Linux.    
Our expectation was that the explanation would come from the difference in idle behavior between the operating systems
%At first glance one suspects that the clear explanation must come from a difference in idle behavior.  

While true, it turns out that it was not in the way we expected.  
Examining 6a, a time line plot of the energy consumed on a representative core between (1 ms separated) interrupts, we see two interesting facts.  
First, across systems a core seems to, as expected, switch between two energy states, high  and low -- busy and idle respectively.  
Second, the library OS's best EPP emerges in such a way that the energy it consumes when busy is actually lower than the energy of Linux Tuned consumes when idle and on the lower end of Linux default.  

One would assume reducing the number of instructions  would naturally open up idle time to be spent in a halted state.  
But given the interplay between the various constants associated with the processor sleep states and the saving had from slowing the processor down it actually pays to not use a configuration that results in a race-to-halt strategy.  
Rather given the library OS simple idle behavior (halt to C7) best EPP results from slowing the processor down and reducing the cost of busy consumption despite it lengthening the busy time which of course comes at the cost of not halting.  

\vspace{-.2in}
\paragraph{Impact of Complexity (Q4):} Fundamentally, it is the headroom in the number of required instructions to do the work that allows this response to adjusting the RAPL and DVFS settings.
Linux on the other hand despite having complex halt algorithms settles on its best EPP when the processor is run at its maximum power setting during the busy time, in contrast to the library OS which runs the processor in its lowest setting during busy intervals.
Thus Linux races to halt, halting many more times, for a shorter period but with at a lower sleep state. 

Overall, we see is that the reduced path length of the libOS has both a direct result ({\bf Q1}) on energy, and provides the libOS the opportunity to slowdown, exploiting an unexpected  ({\bf Q3}) strategy that achieves better performance and energy. 
We hypothesize that Linux does not seem to be able exploit this same strategy, in part because its path lengths are too long, and in part because its sophisticated energy halt behavior ({\bf Q4}) that constantly changing halt state, while in a small time scale having strong advantages over the simple libOS policy, interfere with it adopting a steady state globally more optimal performance.  


\vspace{-.2in}
\paragraph{Impact on Applications (Q2):}
While it is interesting to see the effects of reducing the code path lengths, and how this interacts with energy consumption, is there any real impact from the OS design or implementation choices we make on application efficiency.  To probe this we examine the application centric workload, NodeJS and Silo-TPCc.  

Across these workloads our sweep of ITR, DVFS and RAPL reveals that the OS has significant impacts despite the application centric nature.  
When configured for best EPP we find that the library OS results in a 30\% and 25\% reduction in Joules and time respectively. Which translates to a 48\% EPP improvement.    
For Silo-TPCc, again configured for best EPP, we observe at the highest load we evaluate (600 KQPS) a 19\% and 20\% saving in energy and tail latency.  
In terms of EPP this is 35\% reduction.  
As we reduce load the EPP savings drops to 14\% and 1\% for 100 KQPS and 40 KQPS.  

Surprisingly, the OS must be having an impact on the busy component despite the busy time being dominated by the application.  
After all, NodeJS is closed loop and as such its inter-arrival will decrease as you increase performance and in the case of Silo-TPCc we see the improvements in EPP go up as a function of load.  

Again we will use details from one of the experimental runs to explore this effect.  
This time we will use the closed-loop workload of NodeJS to provide a contrast in our analysis (Figures 3c, 4c, 5c and 6c).  

In figure 3c, given that the time to completion is our performance metric the energy timeline reveals directly the EPP achieved at this setting. 
As such we can see the how the systems compare in both the energy and performance domain as the experiment runs.  
The reduction in slope between the Library OS and Linux tuned demonstrates that at the settings for best EPP (denoted by the labels at the the final point of each run) a lower rate of energy consumption.  

Unlike Memcached, we do not see a complete inversion in the hardware energy settings.  
Best EPP for Linux is achieved when the processor is at its highest values and for the library OS, while it is not the highest, it is a medium s6 DFVS setting (6 highest out of the possible 17). 
As we proceed we explore a fascinating behavior that arises as one sweeps energy setting on the Library OS.

Unlike the OS centric workloads the application centric, as expected, do not display a particularly discrepancy in the number of instructions required to accomplish the requisite work as illustrated in the metrics plots (4c and 4d).  
Surprisingly, however, we see an inversion in how CPI compares in the application centric workload versus the OS centric, across the two Operating Systems.  
Despite the fact that the instructions are application dominated CPI is actually improved in the Library OS despite the fact that the best EPP is being realized at a lower energy setting than Linux.  
Whereas the setting for Linux that achieves best EPP is running the processor at its highest setting to achieve the best performance and energy.

Close loop scenarios, with a fixed amount of work to accomplish, like  our NodeJS and Netpipe workloads, create a scenario which optimizing for time and energy can be roughly equivalent.  
Speeding up the work minimizes the time to receive your next packet upto the limit of transmission and processing needs on the other side.  In the case of NodeJS the transmission time is negligible and the other side's processing cost is also small.   


The library OS's structure results in an interesting modulation between two behaviors ({\bf Q3}).  
When we examine the number of interrupts that this workload induces across the operating systems as we vary the settings (this data has been removed in the interest of space).  
Linux largely executes 200K interrupts for all but the lowest settings of ITR at which point it suffers 300K interrupts.  
Examining our detailed timelines from several runs we see that the NIC asserts and interrupt for every received packet and a send acknoledge interrupt.  
However at the low ITRs the card fires a spurious interrupt such that every request introduces 3 interrupts. 

In the case of libOS when we examine the number of interrupts that occur we find a fascinating behavior.  
Despite this being a closed loop the OS is able to at different energy settings change its behavior ({\bf Q3}) between polling and interrupt driven IO.  

Given the nature of the execution model the event that handles an inbound request synchronously executes the application logic in NodeJS.  
This logic, imitates the send, which the device driver synchronously initiates, and then the application logic continues the expensive completion of the interpreted NodeJS logic.
Given the minimal transmission and processing time, when the application code completes, it returns back to the device logic, whose internal poll loop discovers a new packet available.  
This specific behavior arises when we slow the processor down just to the right speed.  
This is another expected emergent adaptation of the operating system ({\bf Q3}) to optimize energy and performance. 

% net cycles savings is 35% but our time and energy savings are 

% save more energy than time 
% best perf cpi: 
%           halts:
% best epp  cpi:    
%           halts:



