\subsubsection{Can the OS help Application Energy consumption?}

So while it is interesting to see the effects of reducing the code path lengths and how this interacts with energy consumption is there any real impact due to the OS design or implementation choices we make in our software.  To probe this we examine the application centeric workload, NodeJS and Silo-TPCc.  

Across these workloads if our sweep of ITR, DVFS and RAPL reveals that the OS has significant impacts despite the Application centric nature.  When configured for best EPP we find that the library OS results in a 30\% and 25\% reduction in Joules and time respectively. Which translates to a 48\% EPP improvement.    For Silo-TPCc, again configured for best EPP, we observe at the highest load we evalute (600 KQPS) a 19\% and 20\% saving in energy and tail latency.  In terms of EPP this is 35\% reduction.  As we reduce load the EPP savings drops to 14\% and 1\% for 100 KQPS and 40 KQPS.  

Surprisingly, the OS must be having and impact on the busy component despite the busy time being dominated by the application.  After all NodeJS is closed loop and as such its inter-arrival will decrease as you increase performance and in the case of Silo-TPCc we see the improvements in EPP go up as a function of load.  

Again we will use details from one of the experimental runs to explore this effect.  This time we will use the closed-loop workload of NodeJS to provide a contrast in our analysis (Figures 3c, 4c, 5c and 6c).  

In figure 3c, given that the time to completion is our performance metric the energy timeline reveals directly the EPP achieved at this setting. As such we can see the how the systems compare in both the energy and performance domain as the experiment runs.  The reduction in slope between the Library OS and Linux Tuned demonstrates that the settings for best EPP (denoted by the labels at the the final point of each run).  

Unlike the Memcache we do not see a complete inversion in the hardware energy settings.  Best EPP for Linux is achieved when the processor is at its highest values and for the library OS, while it is not the highest, it is a medium s6 DFVS setting (6 highest out of the possible 17). As we proceed we explore a fascinating behaviour that arises as one sweeps energy setting on the Library OS.


Unlike the OS centric workloads the application centric, as expected do not display a particularly discrepency in the number of instructions required to accomplish the requisite work as illustrated in the metrics plots (4c and 4d).  

Surprisingly, however, we see an inversion in how CPI compares in the application centric workload versus the OS centric, across the two Operating Systems.  Despite the fact that the instructions are application dominated CPI is actually improved in the Library OS despite the fact that the best EPP is being realized at a lower energy setting than Linux.  Where as the setting for Linux that achieves best EPP is running the processor at its highest setting to achieve the best performance and energy.

Close loop scenarios, with a fixed amount of work to accomplish, like  our NodeJS and Netpipe workloads, create a scenario which optimizing for time and energy can be roughly equivalent.  Speeding up the work minimizes the time to receive your next packet upto the limit of transmission and processing needs on the other side.  In the case of NodeJS the transmission time is negligible and the other side's processing cost is also small.   


The library OS's structure results in a interesting modulation between two behaviors.  When we examine the number of interrupts that this workload induces across the operating systems as we vary the settings (this data has been removed in the interest of space).  Linux largely executes 200K interrupts for all but the lowest settings of ITR at which point it suffers 300K interrupts.  

Examining our detailed timelines from several runs we see that the NIC asserts and interrupt for every recived packet and a send acknolwedge interrupt.  However at the low ITRs the card fires a suprious interrupt such that every request introduces 3 interrupts. 

In the case of EbbRT when we examine the number of interrupts that occur we find a fascinating behaviour.  Despite this being a closed loop the OS is able to at different energy settings change its behaviour between polling and interrupt driven IO.  

Given the nature of the execution model the event that handles an inbound request is syncronously executes the application logic in NodeJS.  This code 

% examining the energy per 1 ms seperated interrupts we see something quite different.  Largely there is really only one state -- high energy that is being exploited.  This is cooberated by the fact there are few if any halts executed 





These surprising result suggest that, contrary to conventional understanding, operating system behavior can have substantial impact on the efficiency of even the time spent in application compute.  
That is, removing boundary crossing, running application work to completion, and dispatching application code directly from an interrupt, may have a secondary effect on the efficiency of that applciation code that was not previously observed.




