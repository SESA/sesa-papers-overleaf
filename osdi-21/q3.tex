\subsection{Application Light}
\label{sec:q3}
\begin{itemize}
\item Q1: Impact of OS paths?
\item Q4: How do different tuning impact different OSes?
\end{itemize}

% What if any impact does OS path length and hardware tuning have on energy consumption?
%Application specific libOS' written for datacenter workloads often focuses on optimizing packet processing path lengths~\cite{ix, arrakis, ebbrt, farm}. Netpipe and memcached are two examples where the majority of processing is in the OS packet receive paths, given that the application compute portion is simpler in both. Examining the differend offered loads in both workloads, we find that: 1) across all four message size in netpipe, libOS used 6\%-77\% fewer instructions than Linux, and lowered its EPP by 37\%-80\%, and 2) across all three QPS in memcached, libOS used 71\%-75\% fewer instructions than Linux, and lowered its EPP by 41\%-74\%. 

Shorter path length in libOS allows it to expose gaps in packet inter-arrival rates in order to save more energy by further reducing its processor frequency and take advantage of halting. In netpipe 64 KB, figure~\ref{fig:metrics_netpipe64} shows libOS uses 67\% fewer instructions than Linux,  even though figure~\ref{fig:jl_netpipe64} shows that the libOS is able to use a lower ITR delay of 6 \micro s versus tuned Linux's of 12 \micro s, libOS is actually still less busy than Linux tuned (figure~\ref{fig:busy_netpipe64}). This instruction efficiency allows libOS to set DVFS at a lower frequency of 0xc00 versus tuned Linux's of 0x1400 to further reduce its energy consumption. In contrast to libOS, tuning Linux mainly uses similar amount of instructions as default or more instructions. Figure~\ref{fig:metrics_netpipe64} shows tuning Linux uses 17\% more instructions while having a lower EPP by 16\% as compared to default. This is partly due to the 40\% increase in interrupts for Linux, which adds additional interrupt handling instruction overhead. While these additional instructions and interrupts cause Linux tuned to be more busy (figure~\ref{fig:busy_netpipe64}), the data in figure~\ref{fig:joule_netpipe64} indicate tuning Linux consumed less energy. We hypothesize this could be an effect of the ITR delay value affecting Linux's scheduler such that tuned Linux was able to call halt instruction 39\% more than Linux default.

Given that memcached is a less compute intensive workload, it is not a surprise that reducing tail latency is more important for both tuned Linux and libOS. However, what is surprising is the different combination of ITR, DVFS, and RAPL settings that each system took. The libOS used the lowest ITR value possible (2 \micro s) and set its DVFS/RAPL at the median ranges, and while this resulted in a 50\% increase in number of interrupts compared to Linux default, the general efficiency and smaller size of the libOS can be seen in figure~\ref{fig:metrics_mcd600} as even with the extra interrupt counts, the libOS' instruction count was 65\% less than Linux. This efficiency enables libOS to minimize its 99\% tail latency while using less energy. Even though figure~\ref{fig:metrics_mcd600} also shows that the libOS went to C7 sleep state over 2X more than Linux default, we do not believe it actually stayed in those a C7 sleep state given its exit latency (hundreds of \micro s) and the dynamic nature of the workload with such a low ITR delay that will constantly wake up the processor. The biggest indication of tuned Linux's difference from default Linux is in the interrupt count, which is 30\% less than Linux default, we attribute this to its ITR delay value of 30 \micro s. This ITR delay seems to shift the ratio of sleep states to focus on C1 and C1E, overall it seems tuning Linux allows it to halt more often than Linux default. Figure~\ref{fig:metrics_mcd600} supports this by showing that tuning Linux spent less time being busy than default.

%Tuned linux has lower CPI than default, an indication of why it managed to achieve lower tail latency. While tuned Linux was able to be scheduled to sleep more often, it consumed energy at a higher rate than default, could be the exit latencies of more sleep states given the dynamic nature of memcached workload.

%
